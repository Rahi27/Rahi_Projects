{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNwXHLt8IKsckO5FdgxFQ6H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rahi27/Rahi_Projects/blob/main/Images_comp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4UM81WlF-yzv"
      },
      "outputs": [],
      "source": [
        "# === Siamese embedding + inference (MobileNetV2 + contrastive loss) ===\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 1) config ----------\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "MARGIN = 1.0  # contrastive margin\n",
        "ZIP_PATH = \"/content/archive.zip\"          # your zip (if exists)\n",
        "EXTRACT_PATH = \"/content/archive\"          # target extract folder\n",
        "CSV_PATH = \"/content/sketch_photo_dataset(1).csv\"\n",
        "PHOTOS_FOLDER = os.path.join(EXTRACT_PATH, \"photos\")\n",
        "SKETCHES_FOLDER = os.path.join(EXTRACT_PATH, \"sketches\")"
      ],
      "metadata": {
        "id": "76Id6axc_ATi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 2) extract zip if needed ----------\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/archive.zip\"\n",
        "extract_path = \"/content/archive\"\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"✅ Files extracted to:\", extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUaNH7zt_BR4",
        "outputId": "0be2a0b2-41d1-4a11-c718-5df1b12ac510"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Files extracted to: /content/archive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 3) load CSV and prepare splits ----------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"CSV sample:\\n\", df.head())\n",
        "\n",
        "\n",
        "# map labels if needed (adjust mapping to your CSV)\n",
        "label_map = {'yes': 1, 'no': 0}   # change if your labels differ\n",
        "if df['label'].dtype == object:\n",
        "    df['label'] = df['label'].replace(label_map)\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
        "print(\"Train/test sizes:\", len(train_df), len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNq3yGinAWB9",
        "outputId": "365beba4-e75b-49e9-d5d6-4e84ff50e7e3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV sample:\n",
            "      photo_image       sketch_image label\n",
            "0  m1-024-01.jpg  m1-037-01-sz1.jpg    no\n",
            "1  m1-024-01.jpg   m-079-01-sz1.jpg    no\n",
            "2  m1-024-01.jpg   m-089-01-sz1.jpg    no\n",
            "3  m1-024-01.jpg   m-075-01-sz1.jpg    no\n",
            "4  m1-024-01.jpg  F2-018-01-sz1.jpg    no\n",
            "Train/test sizes: 28275 7069\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-690002582.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df['label'] = df['label'].replace(label_map)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 4) image preprocess (with light augmentation option) ----------\n",
        "def load_and_preprocess(path, augment=False):\n",
        "    try:\n",
        "        img = load_img(path, target_size=IMG_SIZE)\n",
        "        img = img_to_array(img) / 255.0\n",
        "    except Exception as e:\n",
        "        # fallback blank image\n",
        "        print(\"Error loading:\", path, \"->\", e)\n",
        "        img = np.zeros((*IMG_SIZE, 3), dtype=np.float32)\n",
        "\n",
        "    if augment:\n",
        "        # simple augmentation: random flip + brightness jitter\n",
        "        if np.random.rand() < 0.5:\n",
        "            img = np.fliplr(img)\n",
        "        b = 0.9 + 0.2 * np.random.rand()\n",
        "        img = np.clip(img * b, 0.0, 1.0)\n",
        "    return img.astype(np.float32)"
      ],
      "metadata": {
        "id": "qEQEmtwLAcKB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 5) generator yielding (sketch_batch, photo_batch), labels ----------\n",
        "def pair_generator(df, batch_size=BATCH_SIZE, augment=False):\n",
        "    n = len(df)\n",
        "    while True:\n",
        "        df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
        "        for start in range(0, n, batch_size):\n",
        "            end = min(start + batch_size, n)\n",
        "            batch = df_shuffled.iloc[start:end]\n",
        "            A, B, Y = [], [], []\n",
        "            for _, row in batch.iterrows():\n",
        "                sketch_path = os.path.join(SKETCHES_FOLDER, row['sketch_image'])\n",
        "                photo_path = os.path.join(PHOTOS_FOLDER, row['photo_image'])\n",
        "                A.append(load_and_preprocess(sketch_path, augment=augment))\n",
        "                B.append(load_and_preprocess(photo_path, augment=augment))\n",
        "                Y.append(row['label'])\n",
        "            yield (np.array(A), np.array(B)), np.array(Y, dtype=np.float32)\n"
      ],
      "metadata": {
        "id": "wtKqygHUAgIa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 6) build embedding network (shared) ----------\n",
        "def build_embedding_model(input_shape=(*IMG_SIZE, 3), embedding_dim=128):\n",
        "    base = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=input_shape)\n",
        "    for layer in base.layers:\n",
        "        layer.trainable = False\n",
        "    x = base.output\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(embedding_dim)(x)\n",
        "    # L2 normalize embeddings so cosine similarity is just dot product\n",
        "    x = Lambda(lambda t: K.l2_normalize(t, axis=1), name='l2_norm')(x)\n",
        "    return Model(inputs=base.input, outputs=x, name='embedding_net')\n",
        "\n",
        "embedding_net = build_embedding_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhJAhDTTAjiE",
        "outputId": "1e4ff9a1-af25-4eb0-b6bb-ee77ca97e9a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_128_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------- 7) siamese model that outputs euclidean distance ----------\n",
        "inp_a = Input(shape=(*IMG_SIZE, 3))\n",
        "inp_b = Input(shape=(*IMG_SIZE, 3))\n",
        "emb_a = embedding_net(inp_a)\n",
        "emb_b = embedding_net(inp_b)\n",
        "\n",
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
        "\n",
        "distance = Lambda(euclidean_distance, name='euclid_dist')([emb_a, emb_b])\n",
        "siamese_model = Model(inputs=[inp_a, inp_b], outputs=distance)"
      ],
      "metadata": {
        "id": "sSftOZvGAmWt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 8) contrastive loss ----------\n",
        "def contrastive_loss(margin=MARGIN):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, y_pred.dtype)\n",
        "        # y_true==1 -> positive pair -> minimize d^2\n",
        "        # y_true==0 -> negative pair -> minimize max(margin - d, 0)^2\n",
        "        pos = y_true * tf.square(y_pred)\n",
        "        neg = (1 - y_true) * tf.square(tf.maximum(margin - y_pred, 0.0))\n",
        "        return tf.reduce_mean(pos + neg)\n",
        "    return loss\n",
        "\n",
        "siamese_model.compile(optimizer=Adam(1e-4), loss=contrastive_loss(MARGIN))\n",
        "siamese_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "rDaj9llIAqGU",
        "outputId": "fa139e80-ef30-4db1-e14f-63bc894ebb55"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_net       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │  \u001b[38;5;34m2,618,816\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ euclid_dist         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ embedding_net[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │ embedding_net[\u001b[38;5;34m1\u001b[0m]… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_net       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,618,816</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ euclid_dist         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_net[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │ embedding_net[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,618,816\u001b[0m (9.99 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,618,816</span> (9.99 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m360,832\u001b[0m (1.38 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">360,832</span> (1.38 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 9) train ----------\n",
        "train_gen = pair_generator(train_df, batch_size=BATCH_SIZE, augment=True)\n",
        "val_gen = pair_generator(test_df, batch_size=BATCH_SIZE, augment=False)\n",
        "steps_per_epoch = max(1, len(train_df) // BATCH_SIZE)\n",
        "val_steps = max(1, len(test_df) // BATCH_SIZE)\n",
        "\n",
        "history = siamese_model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=val_steps,\n",
        "    epochs=EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI-Z6AGAAtki",
        "outputId": "90afda8a-a4d2-4abc-eef8-230967022984"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m883/883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 92ms/step - loss: 0.0035 - val_loss: 0.0153\n",
            "Epoch 2/3\n",
            "\u001b[1m883/883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 84ms/step - loss: 0.0033 - val_loss: 0.0179\n",
            "Epoch 3/3\n",
            "\u001b[1m883/883\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 85ms/step - loss: 0.0034 - val_loss: 0.0089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 10) build gallery embeddings (photos) ----------\n",
        "photo_files = [f for f in os.listdir(PHOTOS_FOLDER) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
        "print(\"Number of photos in gallery:\", len(photo_files))\n",
        "\n",
        "# batch predict gallery embeddings for speed\n",
        "photo_files = [f for f in os.listdir(PHOTOS_FOLDER) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
        "print(\"Number of photos in gallery:\", len(photo_files))\n",
        "\n",
        "# batch predict gallery embeddings for speed\n",
        "def compute_embeddings(file_list, batch=64):\n",
        "    embeddings = []\n",
        "    names = []\n",
        "    for i in range(0, len(file_list), batch):\n",
        "        batch_files = file_list[i:i+batch]\n",
        "        imgs = []\n",
        "        for fname in batch_files:\n",
        "            p = os.path.join(PHOTOS_FOLDER, fname)\n",
        "            imgs.append(load_and_preprocess(p, augment=False))\n",
        "        imgs = np.array(imgs)\n",
        "        embs = embedding_net.predict(imgs, verbose=0)   # normalized embeddings\n",
        "        embeddings.append(embs)\n",
        "        names.extend(batch_files)\n",
        "    return np.vstack(embeddings), names\n",
        "\n",
        "gallery_embeddings, gallery_names = compute_embeddings(photo_files)\n",
        "# gallery_embeddings are L2-normalized per-row"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU2sDdfzAwcd",
        "outputId": "2ec67745-7197-40c8-b6d9-de8c4399a6a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of photos in gallery: 188\n",
            "Number of photos in gallery: 188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- 11) matching function using cosine similarity (dot product) ----------\n",
        "def find_best_matches_for_sketch(sketch_path, top_k=5):\n",
        "    s_img = load_and_preprocess(sketch_path, augment=False)\n",
        "    s_emb = embedding_net.predict(np.expand_dims(s_img, axis=0), verbose=0)[0]  # (embedding_dim,)\n",
        "    # cosine similarity since embeddings are L2-normalized\n",
        "    sims = np.dot(gallery_embeddings, s_emb)   # shape (n_photos,)\n",
        "    order = np.argsort(-sims)  # descending\n",
        "    results = [(gallery_names[idx], float(sims[idx])) for idx in order[:top_k]]\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "Q8AmMxdYCtvl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ---------- 12) test on a sketch ----------\n",
        "test_sketch = os.path.join(SKETCHES_FOLDER, \"m-063-01-sz1.jpg\")   # change to your sketch file\n",
        "top_matches = find_best_matches_for_sketch(test_sketch, top_k=5)\n",
        "print(\"Top matches (filename, cosine-score):\")\n",
        "for fn, score in top_matches:\n",
        "    print(fn, score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amrlzkvNCwiP",
        "outputId": "e3056378-1022-47c0-c7af-293939726aa8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches (filename, cosine-score):\n",
            "m-091-01.jpg 0.7470476627349854\n",
            "m-078-01.jpg 0.746284008026123\n",
            "m-088-01.jpg 0.7391812801361084\n",
            "m1-027-01.jpg 0.728235125541687\n",
            "m-064-01.jpg 0.7161684036254883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3VOXYDENC1XO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}